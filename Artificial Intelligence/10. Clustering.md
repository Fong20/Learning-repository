# 10. Clustering
- Clustering group objects into clusters (groups of objects) such that an object is more similar to other objects in the same cluster than the objects in other clusters without the help of labelled data.
- Clustering is used for unsupervised learning

<img width="542" height="241" alt="image" src="https://github.com/user-attachments/assets/6059d4d2-421b-434d-ba30-90e120152faf" />

## K-means clustering
- A form of centroid-based clustering
- centroid = the center of the cluster
- Eg: finding the centroids that best separates the data into k groups.

<img width="563" height="389" alt="image" src="https://github.com/user-attachments/assets/7d5f9b7d-4617-49b5-9874-cefe0153d3d6" />

### Process:
1. define the value of k, which is the number of clusters to be separated into. In this case, the number of clusters will be 3.

<img width="822" height="436" alt="image" src="https://github.com/user-attachments/assets/072d84a8-ac7c-4d07-8358-fbecb9a5c91c" />

2. randomly choose k points from the dataset and assign them as the centroids. The number of clusters = number of centroids

<img width="822" height="436" alt="image" src="https://github.com/user-attachments/assets/45e795ae-72b0-4df0-b897-c1d7497c9003" />

3. calculate the distance from each point to each centroid (we normally use euclidean distance)

4. The data points will be grouped to their respective nearest centroid

<img width="822" height="436" alt="image" src="https://github.com/user-attachments/assets/4f7ccbf4-01b0-4c65-81a6-ffc3709c1f9c" />

5. calculate the new centroid from the grouped points (mean value) and move the centroid to the calculated center. Some of the blue points will turn green now as they are now closer to the green centroid.

<img width="822" height="436" alt="image" src="https://github.com/user-attachments/assets/16207a96-0b16-4ac3-aebd-b4d4d9dff3f8" />

6. repeat the process until,the change in centroids is small

<img width="822" height="436" alt="image" src="https://github.com/user-attachments/assets/ad741d0b-a573-4dbc-b2d3-9fd1a874dfe4" />

## Evaluation of clustering
1. minimum within-cluster (intra-group) variation
2. maximum between-cluster (inter-group) variation

- between cluster = occurs when there is a in between of custers
- “Good” clusters would have small within-cluster variation and large between-cluster variation.
- It is easier to calculate within cluster variation if we have smaller dataset

### Within cluster variation

<img width="845" height="324" alt="image" src="https://github.com/user-attachments/assets/934e508a-ea8b-4327-9255-675f1375b0f6" />

### Between-cluster variation

<img width="845" height="324" alt="image" src="https://github.com/user-attachments/assets/70c32f86-73eb-4d18-95b5-68eb03584032" />

## Pseudo-F statistics
- Is used to maximise between-clusters variation and minimise within-clusters variation
- The higher the value of F is, the better the clustering solution is.

<img width="845" height="324" alt="image" src="https://github.com/user-attachments/assets/f4d55cc0-2134-4373-99ed-90f21847d01a" />

## Best k
To identify the best value of k, for different values of k, we can follow these few steps:
1. perform k-means clustering,
2. calculate the pseudo-F statistics
3. plot the graph of pseudo-F against k
4. identify the best k with the “elbow”-method

## Feature scaling / Normalisation
Normalisation is used when threre are we want to group two different cluster features / dimensons together

### Normalisation methods
1. min-max normalisation

<img width="845" height="324" alt="image" src="https://github.com/user-attachments/assets/fd60893e-40a2-44a1-be20-31fb8cc851d7" />

2. mean-centred
   
<img width="845" height="324" alt="image" src="https://github.com/user-attachments/assets/938f4f6f-1d15-4244-adac-939bdf47372e" />

3. standardisation

<img width="845" height="324" alt="image" src="https://github.com/user-attachments/assets/4c0505bf-85cf-4856-96e0-5d85a879aea5" />

