# 09. Decision Trees
- Decision tree is a tree-like model that represents decisions and consequences.
- In machine learning, a decision tree is generated using a labelled dataset

## Important terms 
- root node (starting point which represents the whole dataset)
- sub node (nodes which are splitted from the root node)
- decision node (sub nodes which are splitted into further sub nodes)
- leaf / terminal node (last node or nodes which cannot be further split)
- parent node
- child node

## Algorithms in decision tree
1. Classification tree
2. Regression tree

<img width="640" height="332" alt="image" src="https://github.com/user-attachments/assets/a2c30223-20a2-4b70-b8e3-38394c5455de" />

## Pure and unpure nodes
- Pure node is defined as a node that contains data from the same class
- Impure node is defined as a node that contains data of different classes

## Classification tree
When a decision treee classifies things into categories, it is known as a classification tree.

### Gini impurity
- Gini impurity is a metric used to identify the node impurity.
- It is used in classification tree
- The lower the Gini Impurity, the better the feature splits the data into distinct categories.

**Formula:**

```math
Gini impurity = 1 - (probability of yes)^2 - (probability of no)^2
```

```math
Weighted average gini impurity = / total 
```

### Spliting criteria for classification tree 
- To determine the input feature (attribute) to be used as the decision node, select the input feature that provides the most effective split.
- The most effective split would be the split that produces the purest child nodes.

#### Process
1. Find the decision nodes
2. Count all the options in the decision node and expand
3. Calculate the gini impurity of the options
4. Calculate the weighted average of gini impurity
5. Choose the lowest weighted average of gini impurity and expand on it

**Example:**

<img width="1623" height="573" alt="image" src="https://github.com/user-attachments/assets/bbe87c9c-e9ae-4024-9cda-d6ea51149307" />

<img width="1623" height="634" alt="image" src="https://github.com/user-attachments/assets/d769b3c9-2ff5-4805-983c-1c3d8264cc51" />

<img width="1623" height="648" alt="image" src="https://github.com/user-attachments/assets/0f77c709-0241-4197-9a38-6af95269ac2b" />

<img width="1623" height="648" alt="image" src="https://github.com/user-attachments/assets/d69c1f8c-63c9-4554-ad00-298aff39101e" />

<img width="1623" height="648" alt="image" src="https://github.com/user-attachments/assets/c1ceb6f0-3e17-44e9-8570-defce4cda806" />

<img width="1623" height="648" alt="image" src="https://github.com/user-attachments/assets/69f94cf0-f0f2-4dd4-b4f3-54f0f2595a41" />

### Splitting criteria for classification tree with numerical attributes
- In cases where the attribute consists of numerically continuous data, the split is defined with a threshold value.
- The threshold for each numerical attribute be chosen based on the threshold value that gives the lowest Gini impurity

<img width="844" height="367" alt="image" src="https://github.com/user-attachments/assets/5a524dc6-cf01-4f3a-868f-dc112aba8f11" />

<img width="844" height="367" alt="image" src="https://github.com/user-attachments/assets/8ab6b707-9021-44d4-b1ae-020138dbaf36" />

<img width="844" height="367" alt="image" src="https://github.com/user-attachments/assets/50f2b7e8-3e8f-4114-a2cf-a9f855079eda" />

<img width="844" height="367" alt="image" src="https://github.com/user-attachments/assets/dc59460f-048d-4b8b-aaaa-670b98b7d61e" />

<img width="844" height="367" alt="image" src="https://github.com/user-attachments/assets/d3558baa-31ec-4126-8b45-4317358554cd" />

<img width="844" height="367" alt="image" src="https://github.com/user-attachments/assets/994b83f3-060e-49d6-b6b3-8515e1114074" />

## Regression tree
- When a decision tree predicts numeric values, it is known as a regression tree
- The same tree-generation algorithm can be used for regression (i.e. numerical, continuous valued target)
- Instead of Gini impurity, variance of the node is calculated to identify the effectiveness of a split.

### Variance 
- Variance represents the dispersion (spread) of the data points in one terminal node.
- low variance = low impurity. Hence, the more effective the split is.
- high variance = high impurity

**Formula:**

```math
Variance, σ2 =
P
(x − μ)2 / n
```

The predicted value of the decision path is the mean of the data points in the leaf node.

### Spliting criteria for classification tree 
1. Find the decision nodes
2. Count all the options in the decision node and expand
3. Calculate the variance
4. Calculate the weighted average of variance
5. Choose the lowerst weighted average of variance and expand on it

## Overfitting
- Overfitting may occur when the depth of the tree is too high, and starts to memorize the training data rather than learning general patterns. This leads to poor performance on new, unseen data.
- Basically, the decision tree is too big and never ending.

### Tree pruning
- Pruning is the process of removing sub-nodes from a decision node. It is done to prevent overfitting.
- This technique reduces the complexity of the tree by removing branches that have little predictive power. It improves model performance by helping the tree generalize better to new data. It also makes the model simpler and faster to deploy.

#### Process
Tree pruning is performed by
1. splitting the tree as deep as possible.
2. Calculate the metrics (Gini impurity or variance) of the leaf nodes.
3. Remove a pair of leaf nodes based on the lowest gini impurity / variance .
4. Calculate the metrics of the new leaf nodes.
5. If the increase in Gini impurity or variance is sufficiently small, the pair of leaf nodes is eliminated, else the pair will remain in the tree.
