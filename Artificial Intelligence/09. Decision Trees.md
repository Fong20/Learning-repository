# 09. Decision Trees
- Decision tree is a tree-like model that represents decisions and consequences.
- In machine learning, a decision tree is generated using a labelled dataset

## Important terms 
- root node (topmost node / beginning of the tree
- sub node (nodes which are splitted from the root node)
- decision node (sub nodes which are splitted into further sub nodes)
- leaf / terminal node (last node or nodes which cannot be further split)
- parent node
- child node

## Algorithms in decision tree
1. Classification tree
2. Regression tree

To determine the input feature (attribute) to be used as the decision node, select the
input feature that provides the most effective split.
The most effective split would be the split that produces the purest child nodes

Pure node is defined as a node that contains data from the same class
Impure node is defined as a node that contains data of different classes

## Classification tree
When a decision treee classifies things into categories, it is known as a classification tree.

### Gini impurity
- Gini impurity is a metric to measure how often a randomly chosen element would be incorrectly identified.
- It is basically used to identify the node impurity
- used in classification tree

**Formula:**

```math
Gini impurity = 1 - (probability of yes)^2 - (probability of no)^2
```

```math
Weighted average gini impurity = / total 
```

### Finding the decision tree split for categorical data
1. Find the decision nodes
2. Count all the options in the decision node and expand
3. Calculate the gini impurity of the options
4. Calculate the weighted average of gini impurity
5. Choose the lowest weighted average of gini impurity and expand on it

**Example:**

<img width="1623" height="573" alt="image" src="https://github.com/user-attachments/assets/bbe87c9c-e9ae-4024-9cda-d6ea51149307" />

<img width="1623" height="634" alt="image" src="https://github.com/user-attachments/assets/d769b3c9-2ff5-4805-983c-1c3d8264cc51" />

<img width="1623" height="648" alt="image" src="https://github.com/user-attachments/assets/0f77c709-0241-4197-9a38-6af95269ac2b" />

<img width="1623" height="648" alt="image" src="https://github.com/user-attachments/assets/d69c1f8c-63c9-4554-ad00-298aff39101e" />

<img width="1623" height="648" alt="image" src="https://github.com/user-attachments/assets/c1ceb6f0-3e17-44e9-8570-defce4cda806" />

<img width="1623" height="648" alt="image" src="https://github.com/user-attachments/assets/69f94cf0-f0f2-4dd4-b4f3-54f0f2595a41" />


### Finding the decision tree split for numerical attributes
split based on the threshold value
calculate the gini impurity
Calculate the weighted average of gini impurity
Choose the lowerst weighted average of gini impurity and expand on it

## Regression tree
- When a decision tree predicts numeric values, it is known as a regression tree
- The same tree-generation algorithm can be used for regression (i.e. numerical, continuous valued target)
- Instead of Gini impurity, variance of the node is calculated to identify the effectiveness of a split.

**Formula:**
Variance, σ2 =
P
(x − μ)2
n
The predicted value of the decision path is the mean of the data points in the leaf node.

low variance = low impurity
high variance = high impurity

Find the decision nodes
Count all the options in the decision node and expand
Calculate the variance
Calculate the weighted average of variance
Choose the lowerst weighted average of variance and expand on it

## Overfitting
- Overfitting may occur when the number of depth is too high, the number of data points in the leaf node is too little. Basically, the decision tree is too big and never ending.
- Tree pruning is commonly used to prevent overfitting.

### Tree pruning
Pruning is the process of removing sub-nodes from a decision node. It is done to prevent overfitting.

#### Process
Tree pruning is performed by
1. splitting the tree as deep as possible.
2. Calculate the metrics (Gini impurity or variance) of the leaf nodes.
3. Remove a pair of leaf nodes based on the lowest gini impurity / variance .
4. Calculate the metrics of the new leaf nodes.
5. If the increase in Gini impurity or variance is sufficiently small, the pair of leaf nodes is eliminated, else the pair will remain in the tree.
