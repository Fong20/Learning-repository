# 09. Decision Trees
- tree that represent consequences
- In machine learning, a decision tree is generated using a labelled dataset

## Important terms 
- root node (topmost node / initial decis
- decision node
- sub node
- leaf / terminal node (last node, node which has no children nodes
- parent node
- child node

## Algorithms in decision tree
1. Classification tree
2. Regression tree

To determine the input feature (attribute) to be used as the decision node, select the
input feature that provides the most effective split.
most effective split == the split that produces the purest child nodes
a pure node: a node that contains data of only single class
an impure node: a node that contains data of different classes

## Classification tree

### Gini impurity
- Gini impurity is a metric to measure how often a randomly chosen element would be incorrectly identified.
- It is basically used to identify the node impurity
- used in classification tree

**Formula:**

Gini impurity, g = 1 −
X
∀i
p2i
where pi is the probability of target class i

### Finding the decision tree split for categorical data
Find the decision nodes
Count all the options in the decision node and expand
Calculate the gini impurity
Calculate the weighted average of gini impurity
Choose the lowerst weighted average of gini impurity and expand on it

### Finding the decision tree split for numerical attributes
split based on the threshold value
calculate the gini impurity
Calculate the weighted average of gini impurity
Choose the lowerst weighted average of gini impurity and expand on it

## Regression tree
- The same tree-generation algorithm can be used for regression (i.e. numerical, continuous valued target)
- Instead of Gini impurity, variance of the node is calculated to identify the effectiveness of a split.

**Formula:**
Variance, σ2 =
P
(x − μ)2
n
The predicted value of the decision path is the mean of the data points in the leaf node.

low variance = low impurity
high variance = high impurity

Find the decision nodes
Count all the options in the decision node and expand
Calculate the variance
Calculate the weighted average of variance
Choose the lowerst weighted average of variance and expand on it

## Overfitting
- Overfitting may occur when the number of depth is too high, the number of data points in the leaf node is too little. Basically, the decision tree is too big and never ending.
- Tree pruning is commonly used to prevent overfitting.

### Tree pruning
Tree pruning is performed by
1. splitting the tree as deep as possible.
2. Calculate the metrics (Gini impurity or variance) of the leaf nodes.
3. Remove a pair of leaf nodes based on the lowest gini impurity / variance .
4. Calculate the metrics of the new leaf nodes.
5. If the increase in Gini impurity or variance is sufficiently small, the pair of leaf nodes is eliminated, else the pair will remain in the tree.
