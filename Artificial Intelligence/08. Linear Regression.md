# 08. Regression
- Regression is a machine learning model that estimates the relationships between numerical and continuous variables.
- The purpose of regression is to perform estimation or prediction
- The output / target data could be either numerical or continuous

## Linear regression
- Linear regression uses the linear function to estimate the relationship between the input and the output.
- It assumes that there is a linear relationship between the input and output, meaning the output changes at a constant rate as the input changes.
- This relationship is represented by a straight line. 
- Linear regression must have two values

## Components in linear regression
1. Independent variable (input)
2. Dependent variable (output)

**Example: Predicting a student's exam score based on how many hours they studied.**
- Independent variable (input) = Hours studied because it's the factor we control or observe.
- Dependent variable (output): Exam score because it depends on how many hours were studied.

The independent variable to predict the dependent variable

### Finding the best-fit line
- In linear regression, the best-fit line is the **straight line that most accurately represents the relationship between the independent variable (input) and the dependent variable (output).**
- The best-fit line will be the one that optimizes the values of m (slope) and b (intercept) so that the predicted y values are as close as possible to the actual data points.

**Equation:**
- y is the dependent variable (output value)
- x is the independent variable (input value)
- m is the slope of the line (how much y changes when x changes). This is also known as weight in machine learning
- c is the intercept (the value of y when x = 0). This is also known as bias in machine learning

```math
y = mx + c 
```

### Goodness of regression line (Minimize error)
- To identify which linear regression line has the better fit, the errors between the the actual target values and predicted target values are calculated using the **loss function / least square method**.
- It does so by **calculating the cost/loss function value between the actual target values and the predicted target values** by **manipulating the m and c values.**
- This method ensures that the line best represents the data where the sum of the squared differences between the predicted values and actual values is as small as possible.

![image](https://github.com/user-attachments/assets/2878255e-a722-420f-b31d-8a6bb2cadaeb)

**Least square method formula**

residual is the difference between the data points and the prediction the line produces. 

```math
Residual = yᵢ− y^ᵢ
```
![image](https://github.com/user-attachments/assets/bfa91348-c2ee-44ad-8d03-f18703866d56)

When the residuals are squared, we will get the squared errors. 

```math
Squared errors = squared residuals = (yᵢ− y^ᵢ)^2
```

![image](https://github.com/user-attachments/assets/3691a837-430c-402b-afdf-4593551dc1a2)

The larger the residuals, the larger the area of the square.

![image](https://github.com/user-attachments/assets/5693721c-cacb-44e7-86df-e0d7ab55f2e0)

When all the squared errors are added together for a given line, we will get the sum of the squared error which is known as the loss function / cost function

![image](https://github.com/user-attachments/assets/7e7495ed-89b3-4a2e-bbd3-b45d526faa23)


![image](https://github.com/user-attachments/assets/72f8e95b-87f1-479d-b8a1-2cc683ce75f7)

The smaller the cost function value, the better the goodness of the regression line. Thus, l2 has the better linear regression line fit.

![image](https://github.com/user-attachments/assets/54b84025-7545-4579-abcf-88a69dec9ef3)

![image](https://github.com/user-attachments/assets/6a5332b2-79c0-4586-9077-596fe0027399)

### Gradient descent
- Gradient descent is an iterative optimisation algorithm that is used to identify the values of the parameters (m and c) that are used to minimise the cost function.
- The **parameters change in the direction of decreasing cost** to **eventually reach the point of minimum cost.**
- The key idea is that, just like walking down a hill, Gradient Descent moves towards the "bottom" or minimum of the loss function, which represents the error in predictions.
- Gradient descent is only performed when the cost value, J is not optimal

![image](https://github.com/user-attachments/assets/8f54d199-9131-423c-b51f-73e2603dc4a1)

![image](https://github.com/user-attachments/assets/8b565b5e-f630-4317-85ab-173061d1a7bf)

**Manipulating the m value**

![image](https://github.com/user-attachments/assets/34dd9450-96f3-48bb-889e-6bb277fe3ffd)

**Manipulating the c value**

![image](https://github.com/user-attachments/assets/a59580db-82ab-41d7-bfce-62ca4d889384)

### Learning Rate
- Learning rate is a important hyperparameter in gradient descent that controls how big or small the steps should be when going downwards in gradient for updating models parameters.
- It is essential to determines how quickly or slowly the algorithm converges toward minimum of cost function.

#### Effect of learning rate, alpha on learning process

**Big learning rate**
- If Learning rate is too big, the algorithm may take huge steps leading overshooting the minimum of cost function without settling. It fail to converge causing the algorithm to oscillate.
- This process is termed as exploding gradient problem.
- big alpha = fast learning rate

**Small learning rate**
- If Learning rate is too small, the algorithm will take tiny steps during iteration and converge very slowly.
- This can significantly increases training time and computational cost especially for large datasets.
- small alpha = slow training rate

**Balanced learning rate**
- reasonable alpha = balance learning rate

![image](https://github.com/user-attachments/assets/407204c5-5f84-4a29-a768-6dad922d13ca)
