# 08. Regression
- Regression is a machine learning model that estimates the relationships between numerical and continuous variables.
- The purpose of regression is to perform estimation or prediction
- The output / target data is numerical / continuous

## Linear regression
- Linear regression uses the linear function to estimate the relationship between the input and the output.
- It assumes that there is a linear relationship between the input and output, meaning the output changes at a constant rate as the input changes.
- This relationship is represented by a straight line. 
- Linear regression must have two values

## Components in linear regression
1. Independent variable (input)
2. Dependent variable (output)

**Example: Predicting a student's exam score based on how many hours they studied.**
- Independent variable (input) = Hours studied because it's the factor we control or observe.
- Dependent variable (output): Exam score because it depends on how many hours were studied.

The independent variable to predict the dependent variable

### Finding the best-fit line
- In linear regression, the best-fit line is the **straight line that most accurately represents the relationship between the independent variable (input) and the dependent variable (output).**
- The best-fit line will be the one that optimizes the values of m (slope) and b (intercept) so that the predicted y values are as close as possible to the actual data points.

**Equation:**
- y is the dependent variable (output value)
- x is the independent variable (input value)
- m is the slope of the line (how much y changes when x changes)
- c is the intercept (the value of y when x = 0)

```math
y = mx + c 
```

### Goodness of regression line (Minimize error)
- To identify which linear regression line has the better fit, the errors between the predicted target values and the actual target values are calculated using the **loss function / least square method**.
- It does so by **calculating the cost/loss function value between the predicted target values and the actual target values** by **manipulating the m and c values.**
- This method ensures that the line best represents the data where the sum of the squared differences between the predicted values and actual values is as small as possible.

![image](https://github.com/user-attachments/assets/2878255e-a722-420f-b31d-8a6bb2cadaeb)

The smaller the cost function value, the better the goodness of the regression line. Thus, l2 has the better linear regression line fit.

![image](https://github.com/user-attachments/assets/6a5332b2-79c0-4586-9077-596fe0027399)

### Gradient descent
- Gradient descent is an iterative optimisation algorithm that is used to identify the values of the parameters that are used to minimise the cost function.
- The parameters change in the direction of decreasing cost to eventually reach the point of minimum cost.
- Gradient descent is only performed when the cost value, J is not optimal

![image](https://github.com/user-attachments/assets/8f54d199-9131-423c-b51f-73e2603dc4a1)

![image](https://github.com/user-attachments/assets/8b565b5e-f630-4317-85ab-173061d1a7bf)

**Manipulating the m value**

![image](https://github.com/user-attachments/assets/34dd9450-96f3-48bb-889e-6bb277fe3ffd)

**Manipulating the c value**

![image](https://github.com/user-attachments/assets/a59580db-82ab-41d7-bfce-62ca4d889384)

### Effect of learning rate, alpha on learning process

big alpha = fast learning rate

reasonable alpha = balance learning rate

small alpha = slow training rate

![image](https://github.com/user-attachments/assets/407204c5-5f84-4a29-a768-6dad922d13ca)
