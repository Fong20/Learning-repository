# 08. K- Nearest Neighbours (KNN)

Types of KNN
- KNN classification
- KNN regression

## Classification
- Purpose is to categorize or classify data into a class / group based on a labelled dataset
- The data is in discrete non-numerical value
- It is a supervised learning which learns from labelled dataset

**Example:**
- Is the current weather categorised as **cloudy, sunny, or windy?**
- Is the traffic condition categorised as **smooth or congested?**
- Is the mobile app a **potential malware** based on its requested permissions?
- Is the object in an image a **chair or a stool?**

<img width="848" height="365" alt="image" src="https://github.com/user-attachments/assets/a52f2cd9-d98b-4ee0-9fde-a086d8e57734" />

### KNN Classification
The data is classified into a class based on the frequency of occurence that the class

**Process**
1. Identify the k points closest to the data point of unknown class p (hence the name of k nearest neighbours)
2. Calculate distance to find the nearest neighbour. The distance can be calculated through several methods such as the euclidean distance, minkowski distance, manhattan distance, chebyshev distance, and hamming distance. Only hamming distance is used for categorical data
3. Identify the most frequent class of the k nearest points.

**Identifying the k value**
- If k is too small, the result is sensitive to noise;
- If k is too large, all points are predicted as the most probable class.
- Thus, the process of choosing the k value is to identify the optimal value of k which maximise the accuracy (minimise the errors), while minimising the computational complexity (i.e. minimising the value of k).

**Accuracy** 

The accuracy can be measured by using
1. Contingency matrix

<img width="776" height="236" alt="image" src="https://github.com/user-attachments/assets/221fec29-013b-498f-8fab-2da473c23f96" />

2. Confusion matrix

<img width="776" height="245" alt="image" src="https://github.com/user-attachments/assets/65dc167d-bd52-41bc-ba8d-6cc261a61216" />

**Identifying the optimal k value**
- x axis = number of neighbours
- y axis = accuracy
- The optimal k value can be identified by using the elbow method which avoids choosing a k value which is too small and too big
  
<img width="702" height="305" alt="image" src="https://github.com/user-attachments/assets/7d7c640c-14f0-4d71-adb2-37223e4a6601" />

**Measuring distance**
given k value
start from the lowest euclidean distance
count the number occurence of the predicted class on the euclidean distance

example: given k value= 5
start counting 5 euclidean distances value starting from the lowest euclidean distance value
count the number occurence of the predicted class on the euclidean distance
the highest number of occurence of the predicted class on the euclidean distance would be the predicted class for k

## KNN Regression
- To use kNN for regression,
1. Identify the k points closest to the new data point p
2. The mean/median of the target value of k nearest points is the predicted value for the data point p

**Identifying the optimal k value**
- To identify the optimal k using the “elbow”-method, instead of accuracy, the root mean square error (RMSE) can be used as the performance measure.

<img width="848" height="365" alt="image" src="https://github.com/user-attachments/assets/3d7d4a02-9974-4bb4-a0ab-004e2a5cace1" />
