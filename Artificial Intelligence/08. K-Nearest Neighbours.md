# 08. K- Nearest Neighbours (KNN)

## Classification
- Purpose is to categorize or classify data into a class / group based on a labelled dataset
- The data is in discrete non-numerical value
- It is a supervised learning which learns from labelled dataset

**Example:**
- Is the current weather categorised as **cloudy, sunny, or windy?**
- Is the traffic condition categorised as **smooth or congested?**
- Is the mobile app a **potential malware** based on its requested permissions?
- Is the object in an image a **chair or a stool?**

<img width="848" height="365" alt="image" src="https://github.com/user-attachments/assets/a52f2cd9-d98b-4ee0-9fde-a086d8e57734" />

**Types of KNN**
1. KNN classification
2. KNN regression

## KNN Classification
KNN (K-Nearest Neighbors) classification is a supervised machine learning algorithm used to categorize new data points based on the majority class of their closest neighbors in a training dataset.

### Process
1. Identify the k points closest to the data point of unknown class p (hence the name of k nearest neighbours)

<img width="1782" height="861" alt="image" src="https://github.com/user-attachments/assets/922fdf3f-00d1-43b7-8be3-d9306057d07b" />

2. Calculate distance from the given point to the nearest neighbours. The distance can be calculated through several methods such as the euclidean distance, minkowski distance, manhattan distance, chebyshev distance, and hamming distance. Only hamming distance is used for categorical data

<img width="1782" height="849" alt="image" src="https://github.com/user-attachments/assets/49097e11-b0d3-4ea1-85d1-c34664eaf43f" />

<img width="844" height="452" alt="image" src="https://github.com/user-attachments/assets/8cb0412a-e981-43a1-be62-a8d14f2fc1ac" />


3. Sort the nearest neighbours of the given point by their distances in ascending orderIdentify the most frequent class of the k nearest points.

<img width="1632" height="856" alt="image" src="https://github.com/user-attachments/assets/35dbe2e7-4f3b-4e3b-b132-c5a9bbc4b67f" />

4. The points are classified through the vote of the neighbours, and the point is then assigned to the class with the most occurence among the k nearest neighbours. In this case, the given point will be classified to the grey class when k = 7.

<img width="1728" height="856" alt="image" src="https://github.com/user-attachments/assets/e79fbbe9-df00-4412-ab2b-74fdd6678853" />

**Extra example:**
- given k value= 5
- start counting 5 euclidean distances value starting from the lowest euclidean distance value
- count the number occurence of the predicted class on the euclidean distance
- the highest number of occurence of the predicted class on the euclidean distance would be the predicted class for the given point

<img width="741" height="313" alt="image" src="https://github.com/user-attachments/assets/684c253b-2a44-445c-92bb-e6e5ee2d34ed" />

### Importance of k value
- It is important to choose an optimal k value
- If k is too small, the result is a low bias with high variance (overfitting)
- If k is too large, the result is a high bias with low variance (underfitting).
- Thus, the process of choosing the k value is to identify the optimal value of k which maximise the accuracy (minimise the errors), while minimising the computational complexity (i.e. minimising the value of k).

<img width="811" height="683" alt="image" src="https://github.com/user-attachments/assets/d158f250-59d7-4b5c-8a25-79f8489f4c0a" />

**Accuracy** 

The accuracy can be measured by using
1. Contingency matrix

<img width="776" height="236" alt="image" src="https://github.com/user-attachments/assets/221fec29-013b-498f-8fab-2da473c23f96" />

2. Confusion matrix

<img width="776" height="245" alt="image" src="https://github.com/user-attachments/assets/65dc167d-bd52-41bc-ba8d-6cc261a61216" />

**Identifying the optimal k value**
- x axis = number of neighbours
- y axis = accuracy
- The optimal k value can be identified by using the elbow method which avoids choosing a k value which is too small and too big
  
<img width="702" height="305" alt="image" src="https://github.com/user-attachments/assets/7d7c640c-14f0-4d71-adb2-37223e4a6601" />

## KNN Regression
- To use kNN for regression,
1. Identify the k points closest to the new data point p
2. The mean/median of the target value of k nearest points is the predicted value for the data point p

**Identifying the optimal k value**
- To identify the optimal k value in KNN regression, the root mean square error (RMSE) can be used as the performance measure instead of accuracy.
- The y axis is now RMSE instead of the accuracy found in the KNN classification

<img width="848" height="365" alt="image" src="https://github.com/user-attachments/assets/3d7d4a02-9974-4bb4-a0ab-004e2a5cace1" />
