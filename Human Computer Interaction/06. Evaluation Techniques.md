# 06. Evaluation Techniques

## Types of Evaluation

### Formative evaluation
- It is done at different stages of development
- To check that the product meet users’ needs
- Focus on the process

### Summative evaluation
- To assess the quality of a finished product
- Focus on the results

**Example:**

When the cook tastes the soup, that’s formative. When the guests taste the soup, that’s summative evaluation.

## Evaluation Paradigm
An evaluation paradigm is an approach that is influenced by particular theories and philosophies

### Types of Evaluation Paradigm

<img width="566" height="380" alt="image" src="https://github.com/user-attachments/assets/e6ea2930-a55e-44c9-9b50-56a5ecb315df" />

### 1.Quick And Dirty
- ‘Quick & Dirty’ evaluation describes the common practice in which designers informally get feedback from users to confirm that their ideas are in-line with users’ needs and are liked.
- Quick & dirty evaluations are done any time.
- The emphasis is on fast input to the design process rather than carefully documented findings.

### 2. Usability Testing
- Usability testing involves recording typical users’ performance on typical tasks in controlled settings.
- As the users perform these tasks they are watched & recorded on video & their key presses are logged.
- This data is used to calculate performance times, identify errors & help explain why the users did what they did.
- User satisfaction questionnaires & interviews are used to elicit users’ opinions.

### 3. Field Studies
- Field studies are done in natural settings
- The aim is to understand what users do naturally and how technology impacts them.

In product design field studies can be used to:
- identify opportunities for new technology
- determine design requirements
- decide how best to introduce new technology
- evaluate technology in use.

### 4. Predictive Evaluation
- Experts apply their knowledge of typical users, often guided by heuristics, to predict usability problems.
- Another approach involves theoretically based models.
- A key feature of predictive evaluation is that users need not be present
- it is Relatively quick & inexpensive

## Evaluation Techniques
There are four evaluation techniques

1. observing users
2. asking users’ their opinions,
3. asking experts’ their opinions,
4. testing users’ performance

### DECIDE Evaluation Framework
There are 6 steps in the DECIDE evaluation framework

1. Determine the goals the evaluation addresses.
2. Explore the specific questions to be answered.
3. Choose the evaluation paradigm and techniques to answer the questions.
4. Identify the practical issues.
5. Decide how to deal with the ethical issues.
6. Evaluate, interpret and present the data.


## Pilot Studies
- Pilot Study is a small trial run of the evaluation plan.
- The aim is to make sure your plan is viable.

**Pilot studies check:**
- that you can conduct the procedure
- that interview scripts, questionnaires, experiments, etc. work appropriately

- It’s worth doing several to iron out problems before doing the main study
- Ask colleagues if you can’t spare real users

## Heuristic Evaluation
- A heuristic is a guideline or general principle or rule of thumb that can guide a design decision or be used to critique a decision that has already been made
- The general idea behind heuristic evaluation is that several evaluators independently critique a system to come up with potential usability problems

There are 10 heuristics which are used to aid evaluators in discovering usability problems

1. Visibility of system status
2. Match between system and the real world
3. User control and freedom
4. Consistency and standards
5. Error prevention
6. Recognition rather than recall
7. Flexibility and efficiency of use
8. Aesthetic and minimalist design
9. Help users recognize, diagnose, and recover from errors
10. Help and documentation
