# 07. Image Compression
- Data compression refers to the process of reducing the amount of data required to represent a given quantity of information.
- Data compression is achieved when one or more of the data redundancies are reduced or eliminated.

**Type of image compression:**
1. Lossless (Remove data redundancy without removing any important data)
2. Lossy (Some data is loss)

![image](https://github.com/user-attachments/assets/7eff09f4-8091-4b15-913d-039dbae91448)

![image](https://github.com/user-attachments/assets/85c28c74-c5b5-44c0-8ced-ebd9a49e25b2)

## Data Redundancy
- The sentences contain “data” that either provide no relevant information or simply restate what is already known.

**Types of redundancy in digital image**
1. Coding Redundancy
2. Interpixel Redundancy
3. Psychovisual Redundancy

### Coding Redundancy

Number of pixels * number of bits allocated for each pixel 

**Huffman Coding**
- Coding redundancy can be eliminated by using variable-length code, and assign the shortest possible code words to the most probable gray levels.
- The Huffman coding is one of the most popular techniques for removing coding redundancies.

**Process**
1. Determine the probabilities of symbols under consideration
2. Arrange the symbols and combining the lowest probabilities symbols into a single symbol.
3. Working back from the combinations to determine the code words.
4. Encode each symbol (in this case, the value of each pixel).


### Interpixel Redundancy
- Generally, the value (magnitude, gray level) of any given pixel can be reasonably predicted from the value of its neighbor.
- Interpixel redundancy occurs when one or more pixels have the same pixel values

![image](https://github.com/user-attachments/assets/e93c2d36-50d1-426c-b690-13212da4d08d)

### Psychovisual Redundancy
- Certain information simply has less relative importance than other information in normal visual processing. This information is said to be psychovisually redundant.
- It can be eliminated without significantly impairing the quality of image perception.
- The elimination of psychovisually redundant data results in a loss of quantitative information, and it is commonly referred to as quantization.
- It is a irreversible operation (visual information is permanently lost), thus, quantization results in a lossy data compression.

## Compression ratio
Defined as the ratio of losing data from compressing the image

Formula:

CR = n1 / n2, where n1 = the number of words before compression and n2 = number of words after compression

## Fidelity Criteria
Quantifying the nature and extent of information loss is highly desirable.

**Types of fidelity criteria:**
1. Objective Criteria
2. Subjective Criteria

## Image Compression Model
- Based on the techniques for reducing or compressing the amount of data required to represent an image. These techniques are combined to form practical image compression systems.

![image](https://github.com/user-attachments/assets/833acc7a-46b4-46cb-b233-df64330784d2)

## Reconstruction of Compressed image

4 informations are required to reconstruct a compressed image:
1. Size of the image
2. Bits / Pixel
3. Scanning Order
4. Divsion factor (number of bits represented by the image

Put the bits back to the array

![image](https://github.com/user-attachments/assets/a58da6f4-fb81-400b-ab86-9314567d8466)

Convert the bits in the array back to decimal and multiply the values by the factor of 2

![image](https://github.com/user-attachments/assets/7d73a471-a40b-4d3e-83ea-2c135ddd20ad)

## Joint Photographic Experts Group (JPEG)
- A popular algorothm which is used to compress images With minimal quality loss and high compression ratio.

**Process:**
1. Divide into 8x8 blocks
2. Discrete Cosine Transform
3. Quantization
4. Entropy Coding

![image](https://github.com/user-attachments/assets/e31ffd24-0008-4279-9c33-119e424903c6)

### Step 1: Divide into 8x8 blocks

![image](https://github.com/user-attachments/assets/09006e4e-9077-46c8-83a9-3b58b8249d9a)

### Step 2: Apply discrete cosine transform (DCT)
- Apply DCT to each of the 8x8 blocks separately
- DCT is used to pack most of the energy into fewer coefficients (or components).

![image](https://github.com/user-attachments/assets/592a34f1-fe00-45d8-b156-0e3d5f9f1354)

![image](https://github.com/user-attachments/assets/4f53d1c3-15fe-4fbb-8a1c-2238852fbccc)

### Step 3: Apply quantization 
- Apply quantization to each of the 8x8 blocks separately
- Divide the DCT coefficients with a number (or a set of predefined numbers) and then round to the nearest integer.
- Larger number provides higher compression ratio, but lower quality.
- Changing the quality factor before compressing the image will cause the values in the normalization matrix to change.
- Using a higher quality factor will scale down the values (less errors), whereas using a lower quality factor will scale up the values (more errors).


![image](https://github.com/user-attachments/assets/c9adf880-2932-46c8-bbc4-742f0923960d)

![image](https://github.com/user-attachments/assets/8a85e9d4-d0f3-447f-8fc5-06b89f0c2a30)

compress = divide

decompress = multiply

Absolute magnitude difference can be identified by comparing the value after decompress with the value before compression

![image](https://github.com/user-attachments/assets/b23edd50-394f-4319-9bd7-f6d93ad2a24a)

Dividing by a larger number actually reduces the quality (higher error). This is because there are more number of zeros

![image](https://github.com/user-attachments/assets/2b56af84-98a8-4d07-8265-255acecb093a)

### Step 4: Entropy coding
- Huffman coding and / or run-length coding can then be used to encode the coefficients.
- With the large number of zeros (highest probability), we can use very little amount of bits to represent those zeros.






