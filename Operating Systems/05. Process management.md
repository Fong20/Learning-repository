# 05. Process management

## Process
  ### Concept of a process
  - Process are **programs which are currently being executed (currently running)**
  - A program becomes a process when standby tasks (interruption) are performed to run the executable file and the executable file is loaded into the memory.
  - Therefore, a process has to be created in order for the program to run
  -  There can be multiple processes from the same program. Although two processes may be associated with the same program, thay are considred as two separate processes (Example: when we run two chrome web browsers, the program code will be identical but the data, heap and stack sessions will be different)
  
  ### Information in a process
  Generally, a process contains the program code and other information. The information can be divided into four sections:
  - text (program code)
  - stack
  - data
  - heap
  
    ### Text
    - Contains the program code
    - It also contains the current activity represented by the value of the program counter
   
    ### Stack
    Contains temporary data, such as function parameters, return addresses and local variables

    ### Data 
    Contains the global variable

    ### Heap 
    Dynamically allocates memory to process during its run time

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/3aa3e89b-deba-41b9-ae87-c311878b6255)

  ### Process states
  As a process is being executed, it changes state. The process could be in one of the following states:
  - New: Newly Created Process (or) being-created process.
  - Ready: After the creation, process moves to the Ready state, i.e. the process is waiting to be assigned to the processor for execution.
  - Run: The process is executed in CPU (only one process at a time can be under execution in a single processor)
  - Wait (or Block): The process is waiting for some events to occur.
  - Complete (or Terminated): The process completed its execution.
  
    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/a1016fdc-8518-4a61-94b4-cfcdb864ef82)
  
  Note:
  - Only one process can be in running state on a single processing core at any instant (
  - If there are additional processes which are waiting to be processed by the processor, it can be in either the ready state or waiting state

## Process control block (PCB)
- A process control block (PCB) is a **data structure used by operating systems to store important information about running processes.**
- PCB is also known as the task control block (TCB).
- It acts as the **identification for each process, each process in the OS is represented by a PCB.**
- The PCB is created and managed by the OS

  ### Components of a PCB
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/4df03e56-108c-4507-bf08-93071b9dad35)

## Process table
The process table is **an array of PCBs, which means it logically contains a PCB for all of the current processes in the system.**

## Process management
Most of the time, we are running multiple processes. Thus, it is crucial to ensure that the processes are **organized efficiently to ensure that the CPU is utilized efficiently (busy all the time)** and **ensure that each process will eventually get its turn to run**

To ensure that the CPU is properly utilzied to execute multiple processes, we use two techniques:

  ### Multi-programming
  - Multi-programming allows the allocation of more than one program to the CPU. This increases CPU utilization as the CPU always has a process to execute and it is not idling The CPU **switch and executes another program when the current program enters the waiting state**
  - For example, multiprogramming is commonly used to keep the CPU busy while the currently running program is doing I/O operations. I/O operations do not utilize the CPU which causes the CPU to be idle. Therefore, multi-programming eliminates this idle time by allowing another program to utilize the CPU which will increase the CPU utilization.
  - However, multi-programming **does not allow the user to interact with all the processes that are currently sitting inside the memory.** The **user can only interact with the process which is currently running.**
  
    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/1c9b95cb-7c63-4e93-af3f-cebdd72e34b2)
  
  ### Time-sharing 
  - In cases where there are many waiting processes in the memory, these waiting processes might not get the chance to be run if the current running process never finish its execution.
  - **Time sharing technique solves this problem by allocating specific processing time (equal time interval) for each process to be executed by the CPU.** If the process cannot be executed finish within the given time, the CPU will move on and execute another process instead.
  - Time sharing is beneficial in running multi-application
  
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/e9a52e55-e503-412a-8842-ec91e2ff692e)

## Process creation and termination

  ### Process creation
  - The new state in the process state is known as process creation
  - A unique process identifier (PID) is assigned to the new process
  - memory space is also allocated to the process to store program code, pcb, stack etc
  
  ### Process termination
  - Process executes the last statement and requests the OS to delete it
  - Resources are then de-allocated by the OS so they can be allocated to other processes which are waiting for the resources.

## Process queueing
- Process ququeing decribes **how the processes are queued for the CPU to execute.**
- A process which is waiting for CPU to execute will be placed into a queue

  ### How it works?
  - Process queueing works based on the scheduling algorithm. Based on the scheduling algorithm, the short-term scheduler selects a process in the ready state for the CPU to execute.
  - A linked list is used to keep track of all processes in a queue

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/05957095-8517-46f5-be14-ddacceff273b)

  ### Types of queue
  - **Job queue** = Consists of all processes in the system
  - **Ready queue** = Processes which are ready to be executed
  - **Device queue** = Processes which are waiting for I/0 device

## Process scheduling
- Describes the life cycle of a process
- A process can migrate among various queues throughout its lifetime, scheduling of process is done through different schedulers.

   **CPU scheduling decisions may take place when a process**
    1. switches from running state to waiting state
    2. is terminated
    3. switches from running state to ready state ( process did not finish executing within the specific time frame)
    4. switches from waiting state to ready state.
       
  ### Queueing diagram
  - A queueing diagram is a **common visual representation of a scheduling process**
  - Rectangle box represents a queue or event
  - Circle represents the resource that serve the queue
  - Arrows indicate the flow of processes

  Example:
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/ddf92047-af5f-4a41-a328-f94f0e56f5f9)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/5896d644-fe34-4194-9fca-72d11c2f0ee5)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/f53264b9-8214-4930-92c3-3af24e0a0a3e)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/ef675d87-e3a1-4a71-ad95-6509d221f3e9)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/60a90309-8660-4be7-b1ca-72e1c0957039)

  ### Types of schedulers
  The scheduling process can be handled by different schedulers
  - **short-term:** Selects a process from ready queue to be executed by the CPU
  - **long-term:** Select a process to be brought to ready queue
  - **medium-term:** Determine processes which should be terminated or resumed (Eg: crashed program)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/dfeaa186-eb97-4182-8e52-51d9cdca9c37)

    ### Short-term schedulers
    - A short-term scheduler's main role is to **ensure that the CPU is constantly utilized effectively and efficiently**.**It does so by controlling which processes are allocated to the CPU to be executed through the selection of processes which are in ready queue**
    -**When the process is executed, it continues to operate until it either completes its work or runs into an I/O activity that blocks it.**
    - The short-term scheduler can employ a variety of scheduling methods, such as FCFS, SJF, Priority scheduling and round robin with each of them having their own advantages and disadvantages.
 
    ### Medium-term scheduling
    - **Medium-term scheduling is used to manage processes which are blocked or in waiting state.** These processes are currently not running, but they are still awaiting the occurrence of an event in order to start running. **Medium-term scheduler determines which of these blocked processes should be unblocked and allowed to continue running**
    - Moreover, it also **swaps out inactive processes to create memory space for others**
    - The **contents of the suspended process will be stored back to the hard disk** which **allows the process to be reintroduced into the memory and continue execution from where it left off.**

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/d5066936-35a2-480c-a271-95586a4e76bb)

    ### Long-term scheduler
    - A long-term scheduler is an operating system component that determines which processes and when it should be admitted to the system. It does so by selecting processes to be brought to ready queue.
    - It executes much less frequently compared to short-term scheduler as it operates at a higher level and does not make scheduling decisions in real time.
      
    ### Short-term vs Long-term scheduler
    - There needs to be a balance between I/O bound process nad CPU-bound process
    - I/O bound process spends more time doing I/O than computations whereas CPU-bound process spends more time doing computations
    - If there are too many I/O bound processes, the ready queue will be almost empty and the short-term scheduler will have only few processes to select
    - If there are too many CPU bound processes selected, the device queue will be almost empty and the devices will have no work to do

    ### Dispatcher
    A dispatcher **gives control of the CPU to the process selected by the short-term scheduler**
  
    These tasks include:
    - perform context switching
    - switching back to user mode
    - jump to the proper location in the user program to start or restart the program

     ![image](https://github.com/Fong20/Learning-repository/assets/150316121/1dd25ff8-bed8-4c0c-8375-22cb4872d596)
  
  ### Scheduling criteria
  Scheduling criterias are **criterias to be taken into consideration when selecting the algorithm to be used.**

    ### Criterias
    - CPU utilization (Ensure that the CPU is kept busy all the time)
    - Throughput (How many processes which can be executed by the CPU in one second)
    - Turnaround time (Time from the submission of a process at ready queue to the time of completion)
    - Waiting time (Time in which a process has been waiting in the ready queue)
    - Response time (Time it takes from when a request was submitted to the first response provided)
    - burst time = execution time

   - The more criterias we want to meet, the more complicated the calculation will be.
   - Generally, we want to **increase CPU utilization and throughput** while **minimizing turnaround time, waiting time and response time.**
  
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/4cc57cc8-ed24-4313-b553-6861746354b7)
  
    ### Steps to calculate algorithm
    - **Draw a gantt chart to show the sequence of the processes**
    - **calculate the waiting time and turnaround time**
    - waiting time = start time - arrival time
    - turnaround time = completion time - arrival time

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/34c404b7-965a-4f37-bec9-75edd6caf8ba)

## Scheduling algorithm
- first come first served (FCFS)
- shortest job first (SJF)
- priority scheduling (Pr)
- round robin (RR)

  ### First come first serve (FCFS)
  - FCFS is the simplest CPU scheduling algorithm that schedules according to arrival times of processes.
  - The first come first serve scheduling algorithm states that the process that requests the CPU first is allocated the CPU first.

   **Example 1: Assuming that the processes arrive in the order P1, P2 and P3 and the arrival time is the same for all processes.**
  
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/5e749c60-f356-4e6e-a241-56f6b1e177a7)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/a63e24ca-b956-46d5-b259-25d5cdbb872a)
  
  **Example 2: Processes arrive in different order, P2, P3 and P1 but the arrival time for each process is still the same.**

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/fd58f2dc-44c8-4e2f-b981-dd57b956b312)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/b41ab522-a45d-43a4-9068-df95d586703c)

  **Important observation:**
  - Based on the results calculated from example 1 and example 2, example 2 is much efficient than example 1 as it has lower average waiting time and turnaround time.
  - This is due to the **scheduling of processes.** The **process with the shortest burst time must be at scheduled at the front** whereas the **process with the longest burst time must be arranged at the back when the arrival time is the same**
  -  **Example 1 experiences convoy effect**, whereby **the short process is arranged behind the long process.** This will **cause longer waiting time and turnaround time of the short processes.** Not only that, the short process at the back might not get executed. 

  **Example 3: Processes with different arrival time**
  
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/ac2850e8-e8d7-402a-8369-e573bb6cfe56)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/fe6b8913-c1c0-413b-9292-75c5f7497e20)

  ### Shortest Job First (SJF)
  - **Processes are scheduled to be executed based on its burst time. The process with the shorter burst time will be executed first**
  - Uses the next CPU burst time to schedule the process with the shortest time
  - SJF's advantage is **its minimum average waiting time among all other schduling algorithms** but **it has difficulties in knowing the exact length (Burst time) of the next CPU request**
  - It can only estimate the length by using the length of the previous CPU burst time.

  **Example 1: All processes have the same arrival time.**

  If the arrival time is equal for all processes, we can just directly schedule the processes to be executed based on the their burst time (shortest burst time to longer burst time)
  
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/1a000ed6-1e99-46a3-944b-60f33d6803e9)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/3718a6c0-f483-4c53-bcd0-c7c859222bcb)


  **Example 3: Processes arrive in different time**

  If there are different arrival times for each process, the **first process to be executed will always be the process with arrival time of 0.** After that, we **schedule the process to be executed after the first process by comparing the burst time of the processes in the waiting queue.**

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/383a92d7-f9ad-44a4-9959-96ca0427fee2)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/f99d4198-2dda-448f-bc0e-d66207b35d17)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/b95f5770-da0c-4ab0-9e0d-b1106219a129)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/6bc16757-c9a7-44b5-9aa3-fe1f136ce3dc)


  ### Priority Scheduling
  - **In priority scheduling, the CPU is allocated to the process with the highest priority.**
  - **Processes with the same priority are executed on a first-come first served basis.**
  - It can be divided into two types: preemptive and non-preemptive
  - The lower the priority number, the higher the priority
  
    ### Types of priority scheduling
  - **Non-preemptive = A running process cannot be interrupted by another process**
  - **Preemtive = A running processs can be interrupted by another process with higher priority**

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/cda26712-6f52-4a6a-8ab1-5967eae18a42)
 
    ### Aging process
    **One of the downsides of priority scheduling is that low priority scheduling processes might not get the chance to be run.** Aging process solves this problem by **increasing the priority of the process as time progresses.**

    **Example 1: Non-preemptive priority scheduling with same arrival time**
    
    If the arrival time is the same, we schedule the processes to be executed based on the priority number.
 
    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/ece570f7-7db2-4667-9d3b-c29f53b975af)
 
    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/30f927d6-e4cf-455f-98e1-93f42fad9bd8)

    **Example 3: non-preemptive case with different arrival times**
    
    If the arrival time is different for each process, the first process will always have arrival time of 0. Once the first process is finished executed, the next process is allocated to the CPU by comparing the priority number of the processes located in the waiting state.

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/925eeb57-d85e-4033-9f24-e54fb47973d7)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/d9d7ea46-5b4e-4771-818a-05d2e98a3e2b)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/a0b602b0-37f9-491d-ad15-c4f7ce82f8e5)

    **Example 4: Preemptive with different arrival time**
    
    The first step is to list down all the process based on the arrival time. Next, check if the priority number is higher than the current first process. If it is higher, then it will run the process with the higher priority first. Otherwise, select the process with the lower priority to be the next process to be run after the first process.

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/589da514-3439-46d8-b478-981f4cda3441)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/489df9b5-a257-4c93-997b-ca0de81d70f3)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/ea6ea5eb-8079-4159-937c-2f112190056d)

### Round Robin (RR)
- **Round robin is using the time sharing process technique** where **each process is allocated a small unit of CPU time (time slice/quantum).** If the time has elapsed, the process will be preempted and added to the end of the ready queue.
- The average turnaround time of round robin is higher than shorter job first (SJF) but it provides better response.

  ### How does it work?
  - Each process or job present in the ready queue is assigned the CPU for that time quantum.
  - If the execution of the process is completed during that time,  the process will terminate
  - Whenever the time slice expires, the existing process will be transitioned from running state back to the ready queue and the next process will be executed.
  - If there are new ready queued processes which arrive much quicker than the unfinished processes, it will be queued before unfinished processes.

  ### Advantages
  The main advantage of round robin scheduling algorithm is **it is not affected by the convoy effect or the starvation problem as occurred in First Come First Serve CPU Scheduling Algorithm since all processes are allocated equal time slice to be executed.**

  ### Disadvantages
  - There is Larger waiting time and Response time.
  - There are potentially more overhead due to increase in context switches.
  
The arrival time of the new process determine the position in the ready queue
![Screen Shot 05-24-24 at 10 59 AM](https://github.com/Fong20/Learning-repository/assets/150316121/603f2cde-6f87-4d20-a1ca-fb986cec85f0)

![Screen Shot 05-24-24 at 11 00 AM](https://github.com/Fong20/Learning-repository/assets/150316121/05e4aa01-c18b-4674-9e22-781197dd0ccb)

![Screen Shot 05-24-24 at 11 08 AM](https://github.com/Fong20/Learning-repository/assets/150316121/c41bad4a-af3f-4258-9b23-4a02d7ba3eef)

**Example 1: Round robin algorithm with equal arrival time and time slice of 4**

![Screen Shot 05-24-24 at 11 19 AM](https://github.com/Fong20/Learning-repository/assets/150316121/f2b16151-227e-4fee-93eb-172b86881f33)

![image](https://github.com/Fong20/Learning-repository/assets/150316121/2ce04aef-699c-4bbf-a2a0-852edfc79071)

**Example 2: Round robin scheduling algorithm with different arrival time and time slice of 4**

![image](https://github.com/Fong20/Learning-repository/assets/150316121/c47b7541-ab6f-449e-a9da-cfec3ce8512b)

  ### Round-robin vs First come first serve (FCFS)
  The round robin scheduling algorithm is equivalent to FCFS when the arrival time is the same for all processes and the short processes are allocated to be executed first followed by the long processes

  ### Optimal quantum time
  - It is crucial to find the optimal quantum time as improper allocation of quantum time will have its consequences.
  - If the time slice is allocated too large, the scheduling becomes such like FCFS
  - If the time slice is allocated too small, there will be a lot of overhead due to increase in context switching.
  - The optimal quantum time is from around 4 to 6 seconds

## Multilevel queue scheduling
- **In multilevel queue scheduling, the processes which are in the ready queue can be further partioned into separate classes where each class can have its own scheduling algorithm based on its importance and urgency of execution**
- The higher-priority processes are placed in queues with higher priority levels, while the lower-priority processes are placed in queues with lower priority levels.
- Every queue would have an absolute priority over the low-priority queues. No process may execute until the high-priority queues are empty.

**Classes of a multilevel queue scheduling**
1. system process = system processes. It has the most foreground and shorter burst time
2. foreground process (interactive) = Consists of processes which are urgent and needed to be executed immediately
3. background process (batch) = It collects programs and data into a batch before processing start. It has the lower priority and a longer burst time

![image](https://github.com/Fong20/Learning-repository/assets/150316121/4f487bb2-cfe3-4401-a505-63a1d271087c)

  ### How it works?
  1. **Fixed priority preemptive scheduling method** = Each queue has absolute priority over the lower priority queue. Let us consider the following priority order queue 1 > queue 2 > queue 3. According to this algorithm, no process in the batch queue(queue 3) can run unless queues 1 and 2 are empty. If any batch process (queue 3) is running and any system (queue 1) or Interactive process(queue 2) entered the ready queue the batch process is preempted.
2. **Time slicing** = In this method, each queue gets a certain portion of CPU time and can use it to schedule its own processes. For instance, queue 1 takes 50 percent of CPU time queue 2 takes 30 percent and queue 3 gets 20 percent of CPU time.

  ### Advantages of multilevel queue scheduling
  1. **Efficient allocation of CPU time:** The scheduling algorithm ensures that processes with higher priority levels are executed in a timely manner, while still allowing lower priority processes to execute when the CPU is idle. This ensures optimal utilization of CPU time.
  2. **Fairness:** The scheduling algorithm provides a fair allocation of CPU time to different types of processes, based on their priority and requirements.
  3. **Customizable:** The scheduling algorithm can be customized to meet the specific requirements of different types of processes. Different scheduling algorithms can be used for each queue, depending on the requirements of the processes in that queue.

  ### Disadvantages of multilevel queue scheduling
  1. There is a risk of starvation for lower priority processes.
  2. It is rigid in nature.

  **Example:**
  
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/64cc89b4-12db-4a17-b865-a2056a477a9e)

## Multilevel feedback queue scheduling
**Multilevel Feedback Queue Scheduling (MLFQ) CPU Scheduling is similar to Multilevel Queue (MLQ) scheduling where it divides processes to multiple queues based on their priority. However, the processes are allowed to move between the queues.**

  ### How it works?
  Suppose that queues 1 and 2 follow round robin with time quantum 4 and 8 respectively and queue 3 follow FCFS.
  
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/91bf9c06-c5c6-4898-9117-11fa4de8dcc0)

  1. When a process starts executing the operating system can insert it into any of the above three queues depending upon its priority. For example, if it is some background process, then the operating system would not like it to be given to higher priority queues such as queues 1 and 2. It will directly assign it to a lower priority queue i.e. queue 3. Let’s say our current process for consideration is of significant priority so it will be given queue 1.
  2. In queue 1 process executes for 4 units and if it completes in these 4 units or it gives CPU for I/O operation in these 4 units then the priority of this process does not change and if it again comes in the ready queue then it again starts its execution in Queue 1.
  3. If a process in queue 1 does not complete in 4 units then its priority gets reduced and it is shifted to queue 2.
Above points 2 and 3 are also true for queue 2 processes but the time quantum is 8 units. In a general case if a process does not complete in a time quantum then it is shifted to the lower priority queue.
4. In the last queue, processes are scheduled in an FCFS manner.
5. A process in a lower priority queue can only execute only when higher priority queues are empty and a process running in the lower priority queue is interrupted by a process arriving in the higher priority queue.

Note: A process in the lower priority queue can suffer from starvation due to some short processes taking all the CPU time. A simple solution would be to increase the priority of all the processes using aging after certain intervals and place them on the higher priority queue 

![image](https://github.com/Fong20/Learning-repository/assets/150316121/b2f64e2e-3898-4c5e-94c6-83940b1dedd1)

  ### Advantages of multilevel feedback queue
  - It is more flexible.
  - It allows different processes to move between different queues.
  - It prevents starvation by moving a process that waits too long for the lower priority queue to the higher priority queue.
  
  ### Disadvantages of multilevel feedback queue
  - It produces more CPU overheads.
  - Complexity. It is the most complex algorithm.

  ### Multilevel feedback queue vs multilevel queue scheduling
  Multilevel feedback queue scheduling enables a process to switch between queues. If a process consumes too much processor time, it will be gradually switched to the lower priority queue whereas a process which is waiting in a lower priority queue for too long may be shifted to a higher priority queue. This type of aging prevents starvation.
  
## Multiple processor scheduling
Multiple processor scheduling or multiprocessor scheduling focuses on designing the system's scheduling function, which consists of more than one processor.

### Approaches to multiple processor scheduling
1. Asymmetric multiprocessing
2. Symmetric multiprocessing

  ### Asymmetric multiprocessing (AMP)
  - **The processors in asymmetric multiprocessing have a master-slave relationship, with one master processor controlling the other slave processors.**
  - The master processor manages the data structure, scheduling of processes, I/O processing, and other system operations.
  - The slave processors may receive specified tasks from the master processor or receive processes from the master processor.
  - If the master processor fails, one of the slave processors assumes control of the execution.

![image](https://github.com/Fong20/Learning-repository/assets/150316121/d20a5aca-a96c-4f7a-9e22-195a04bb6ff1)

  ### Symmetric multiprocessing (SMP)
  Each processor is self-scheduling and uses shared memory to communicate. The processors can either start processes from a shared ready queue, or its own private queue of ready-to-execute programs. The scheduler must ensure that no two CPUs run the same task simultaneously.

![image](https://github.com/Fong20/Learning-repository/assets/150316121/6e347aef-09e4-4a9b-b708-739f65cedcf4)

  ## Load balancing
  - **Load balancing is performed in sysmmetric multiprocessing (SMP) to ensure that the workload is balanced among all processors to fully utilize the benefits of a multiprocessor.**
  - **Load balancing is necessary only on systems where each processor has its own private queue of a process that is eligible to execute.**

  **There are two types of load balancing method:**
  1. **Push migration** = **A specific task perodically checks the load on each processor** and **distributes the load by pushing processes from the overloaded processor to the less busy or idle processors.**
  2. **Pull migration** = **The idle processor will pull a waiting task from a busy processor.**

**Note: The push and pull load balancing method can be implemented in parallel on a system.

## Threads
- A thread is a flow of execution through the process code, with its own program counter that keeps track of which instructions to execute next
- Each thread has its own register and stack but share the same code, data, files
- **If the operating system do not support the usage of threads, the basic unit of CPU utilization would be the process** whereas **if the operating system supports the usage of threads, the basic unit of CPU utilization would be the thread.**

![image](https://github.com/Fong20/Learning-repository/assets/150316121/8c0d8fb5-b4b6-4cf6-b3d7-e60d1eee133d)

  ### Single thread vs multithread
  **Traditional process has only a single thread, so one process can perform only one task at a time.** **If a process has multiple threads, it can perform more than one task at a time.** However, a multiprocessor is required to fully utilize threads
  
  The diagram below shows the difference between single-thread process and multithreaded process
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/6c85e36d-c353-4be6-a238-adffaf6a0e47)

  ### Difference between process and threads
  - Processes are used to occupy the memory space and the resources required
  - Threads are the entities which are scheduled for execution on the CPU (unit of execution)

  ### Components of a thread
  1. Code section
  2. Data
  3. files
  4. Stack
  5. Registers

 **As every process has its own Process Control Block, a thread also has its own Thread Control Block.** The **Thread Control Block (TCB) contains the thread Identifier (TID), program counter, registers, stack, and small control block.**

  **Each thread has its own stack as different threads contain different functions or procedures, and thus a different execution history** whereas **the code section, data section and resources are shared among the other threads belonging to the same process.**

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/47e0eaf6-4eec-47d9-972d-15b753d90727)

**Example: Threads used in browser**
The following diagram showcases multiple threads in a single browser process whereby each of the browser tab is a thread.

![image](https://github.com/Fong20/Learning-repository/assets/150316121/cd4a5709-ee5a-493d-9845-19164c280fda)

![image](https://github.com/Fong20/Learning-repository/assets/150316121/e0f54787-63d7-41be-9c5d-c8edef10d39e)

One of the benefits of threading is that it saves memory space, all 4 tabs can be run in a single process as individual threads instead of running 4 individual browser process which takes up more memory space.
![image](https://github.com/Fong20/Learning-repository/assets/150316121/35dba07f-fa0d-49d1-96dc-40a8d9bc4009)

## Advantages of using threads
1. Responsiveness
2. Resource sharing
3. convenience
4. scalability

## Challenges of using threads
Generally, threads are harder to implemenent as compared to a process

1. Dividing activities
2. Balance
3. Data Splitting
4. Data dependency
5. Testing & Debugging

## Support for threads (thread design model)
There are two main threading models in process management: user-level threads and kernel-level threads.

  ### User level threads (Many to one model)
  - **In the user level thread model, the operating system does not directly support threads.** Instead, **threads are managed by a user-level thread library, which is part of the application.** Thus, switching between threads does not interrupt the kernel
  - Many user threads are mapped to a kernel thread
  - Although there are multiple threads, they are unable to run parallaly. Only one thread can access the kernel at a time by making a system call. Once a thread makes a system call to access the kernel thread, other threads in the same process cannot access the kernel thread.
    
  **Example of user-level thread library**
  - Java
  - Win32
  - POSIX (used for linux)
    
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/2c8c40ae-6d9c-4f2e-a9e2-aab0d52eec48)

  **Advantages**
  1. User level threads are simpler and faster to generate. They are also easier to manage.
  2. Thread switching in user-level threads doesn't need kernel mode privileges.
  3. These are more portable.
  4. These threads may be run on any OS.

  **Disadvantages**
  1. The complete process is blocked if a user-level thread runs a blocking operation.
  2. User-level threads don't support system-wide scheduling priorities.
  3. It is not appropriate for a multiprocessor system.
  
  ### Kernel level threads (one to one model)
  - In kernel level thread, the kernel handles all thread management. Each user thread is mapped to a kernel thread. Thus, each thread can be allocated to each processor to run parallel on mutiprocessors
  - It does not have thread library. The OS provides system calls to create and manage threads
  
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/67003e0c-a495-4fd1-86ec-8f0735c1f9f7)

  **Advantages:**
  1. Better performance: Kernel-level threads are managed by the operating system, which can schedule threads more efficiently. This can result in better performance for multithreaded applications.
  2. Greater parallelism: Kernel-level threads can be scheduled on multiple processors, which allows for greater parallelism and better use of available resources.

  **Disadvantages:**
  1. Less flexibility and control: Kernel-level threads are managed by the operating system, which provides less flexibility and control over thread management compared to user-level threads.
  2. Less portability: Kernel-level threads are more tightly coupled to the operating system, which can make them less portable to different operating systems.
  
## User level thread vs Kernel thread level
1. Users implement the user-level threads. On the other hand, the OS implements kernel-level threads.
2. User-level threads may be created and handled much faster. In contrast, kernel-level threads take longer to create and maintain.
3. The entire process is halted if a single user-level thread carries out a blocking operation. On the other hand, if a kernel thread carries out a blocking operation, another thread may continue to run.
4. The user-level thread library includes the source code for thread creation, data transfer, thread destruction, message passing, and thread scheduling. On the other hand, the application code on kernel-level threads does not include thread management code. It is simply an API to the kernel mode.
5. **User-level threads do not invoke system calls for scheduling.** On the other hand, **system calls are used to generate and manage threads at the kernel level.**
6. **The user-level thread is also referred to as the many-to-one mapping thread, as the OS assigns each thread in a multithreaded process to an execution context.** On the other hand, **One-to-one thread mapping is supported at the kernel level where each user thread must be assigned to a kernel thread. This mapping is handled by the OS.**
7. Context switch time is less in the user-level threads. On the other hand, context switch time is more in kernel-level threads.
8. User-level threads may operate on any OS. In contrast, kernel-level threads are specific to the OS.
9. Multithread applications are unable to employ multiprocessing in user-level threads. In contrast, Kernel-level threads may be multithreaded.
10. Some instances of user-level threads are Java threads and POSIX threads. On the other hand, some instances of Kernel-level threads are Windows and Solaris.

The diagram below shows only one thread can access the kernel thread at a single time

![image](https://github.com/Fong20/Learning-repository/assets/150316121/0849260b-fb98-4acc-841e-9f056d0cf40b)

The diagram below shows that every user thread is mapped to a kernel thread which allows the thread to be allocated to each processor to run parallel on mutiprocessors

![image](https://github.com/Fong20/Learning-repository/assets/150316121/aa20dc51-e263-4dcb-a27d-b1da9c2954bb)

  ### Many to many model
  - In the many to many model, several user-level threads are mapped to several kernel-level threads. The number of kernel threads are created based on a particular application.
  - If any thread makes a blocking system call, the kernel can schedule another thread for execution.
  - Although the many to many model allows the creation of multiple kernel threads, true concurrency cannot be achieved by this model as the kernel can schedule only one process at a time.

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/567c83cd-e288-4304-a0a7-0d6b515a5e01)

## Thread pools
- Thread pools are a collection of pre-created threads which are used for faster access.
- It is much quicker for a service to request for an existing thread as compared to creating a new thread. Once the service has finished its service, the exisitng thread is returned to the pool.

![image](https://github.com/Fong20/Learning-repository/assets/150316121/96b9e161-08d2-4ad6-af05-680191e27b9c)

## Process communication

### Interprocess communication
Inter-process communication (IPC) is a mechanism that allows processes to communicate with each other and synchronize their actions. The communication between these processes can be seen as a method of co-operation between them.

  ### Reasons for interprocess communication
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/78bd2fd6-27d2-4625-ba36-0d235a47f273)

  ### Types of interprocess communication:
  1. Shared memory (memory is shared between processes)
  2. Message Passing (call function to the OS and the OS transfers the function to another process)

  ### Shared memory (buffer)
  The shared memory communication mechanism is often explained using the producer-consumer problem.
  
  **Important terms:**
  - Producer = shares the data
  - Consumer = Consumes the data
  - Buffer = shared memory
    
  In shared memory interprocess communication, processes exchange information and resources through a shared memory or also known as buffer where the item produced by the Producer is stored and from which the Consumer consumes the item if needed.

  In the following diagram, Process A shares memory with Process B to communciate with each other. Process A acts as a producer which shares the data to the shared memory whereas Process B acts as a consumer which consumes the data from the shared memory

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/2a88a058-9e90-4493-8ba7-dbf7ef4818cb)

  **When processes are cooperating, both the producer and consumer must be synchronized.** **Synchronization error/communication fail occurs when the process tries to consume the data from the shared memory but there is no data in the shared memory**
    
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/1e0f8a7a-1cc4-48ef-a5da-d1adc8ef150d)

  ### Race condition
 - **Race condition is a condition which occurs in shared memory communication whereby when two processes are trying to access the same data/resource at the same time** Concurrent access to shared data/resources by processes may result in data inconsistency
 - Race condition also occurs when the producer produces data/resource slower than the rate which the consumer consumes.
 
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/f7fa9f74-c749-4202-93bd-19ae2f0de0d0)

  The solution to solving Race Condition is by locking the variable. By locking the variable, it prevents processes from accessing the resources until the particular process is finished being executed.

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/f927929e-99d8-46e4-bfb9-ec9c37b49b48)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/c55db005-4890-4f39-88f0-8d60061e96a7)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/a0337fac-eee4-44ac-a953-c9422a800bdf)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/ee4a1252-b22e-4ad8-adc6-6ffe97d4a5f5)

  ### Message Passing
  Message passing is another mechanism which is used by processes to communciate. When processes want to communicate, they need to establish a connection and exchange messages through the two operations provided by the message passing module:
  - **send(receiver, message)**
  - **receive(source, message)**
  
  **Size of message**
  1. Fixed size = easier for system implementation but harder for the programmer as the data has to be split into chunks
  2. Variable size = harder for system implementation as the task has to be split but easier for the programmer as there is no need to worry about the size limit.

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/547467e9-ef27-4020-90c3-36606b2b8265)

  **Structure of a message**
  1. header = consists of message type, destination, source, message length etc
  2. body = message which would like to be transferred

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/b8106413-53f8-41a2-b948-49b5bacc965c)

  **Type of communication link**
  1. Direct communication
  2. Indirect communication

  ### Direct communication
  - In a direct communication, a link is established automatically between each pair of process and the link is bi-directional.
  - Each of the processes must be named explicitly

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/5dc3915b-f8de-4735-a7c8-593a653b8bd1)

  ### Indirect communication
  - In an indirect communication, communication between processes is done through a mailbox. Messages are sent and received from mailboxes
  - Indirect communication is used in modern OS
  - Each process do not need to be name explicitly
  - Process A sends data to the mailbox in the kernel. The kernel then sends the data to process B. The mailbox is then destroyed.

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/0ea166f8-c323-4b33-ac18-6fa561f4b81c)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/81850380-539f-4de2-bbd2-f69bd428a295)

  ### Synchronous message passing vs Asynchronous message passing
  - Synchronous message passing means the sender cannot send another messsage until the message is received by the receiving process or by the mailbox.

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/8225fef5-7af6-47b8-8395-d9f7078b1a4a)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/0a3fabee-04bf-4fe4-b5ee-36bc94bd7339)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/880997ac-381e-4604-8be6-45643a8f4d6f)

  - Asynchronous message passing means the sender can send another message without waiting for message to be received by the receiving process or the mailbox.

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/ee7b7aee-9414-48be-b84b-ba0aafc1e27e)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/979d30a5-8733-4d26-93a5-627e0c6cc21c)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/bd23d99d-cb6e-4bd6-b8fb-8c65da02a2f4)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/7cab1861-4198-4fad-a9a2-f2ebbc90a5ac)

    ![image](https://github.com/Fong20/Learning-repository/assets/150316121/e38ca44e-0cb3-4453-9bba-50e64a8af34b)

### Execution order of process

  **Example 1: Sequential execution**

  In sequential execution, the process is executed process by process. Hence, there is no issue arise.

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/ca982251-fc66-42cd-a4ed-8e0b8340a7e4)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/30f280ff-787e-42fb-8046-dbc83aeb5214)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/3ef6e432-a0c6-4fdb-b213-0b6cea9d11b2)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/34865ba5-cc57-4748-98e3-c2bfa7fa91b2)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/0082f355-5210-4241-94e6-14c5f16cbd62)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/4469b42f-1ad5-4a90-a22c-059b31da4e91)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/03c2639b-f49e-46d5-bf03-3d150bbb4dc9)

  **Example 2: Concurrent execution**

  In concurrent execution, both processes run parallely. Both processes are fighting for the counter

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/5d786537-ba6c-4516-ac80-41f1df682b70)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/54838bef-a881-43d4-a062-a8924abd99b8)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/0c8d6307-002f-4afb-8e37-8413cce5d191)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/f2ccc9bc-2094-4d58-b616-c9f478aba548)

## Process synchronization
- **Process synchronization is used to rectify race condition** by **coordinating the execution of processes in such a way that no two processes can have access to the same shared data and resources**
- This involves the usage of programming to rectify the issue

  ### Sections of a program
  There are four essential section in the program
  1. Entry section = Handles the entry of a process
  2. Critical section = allows one process to enter and modify the shared variable
  3. Exit section = checks if the process has finished its execution and allows another process which is waiting in the entry section to enter the critical section.
  4. Remainder section = codes which are not part of the entry section, criticl section and exit section

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/a7db5409-896c-4488-9003-2779e4350295)

  ### Critical section
  - **Critical section is a segment of code which is performed by many threads/process.** The process may change the common variables, update table, write a file and many more. It normally consists of shared data resources which are required to be accessed by other processes.
  - ** The critical section area is vulnerable to a race condition because various outputs from concurrently running threads potentially result in different orders of execution.**
  - **The critical section can only be accessed by a process at a single time. When one process is accessing the critical section, no other process is allowed to access the critical section. This prevents multiple processes from executing the critical section at the same time.**
  - This is achieved whereby each process must request for permission to enter the critical section

  In the following example, the counter = counter + 1 code is known as the critical section
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/280991c4-f266-41fe-a0f0-082653a4e3e5)

  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/44b13381-50be-4f7b-9053-d216eb72ba25)

  **How to control the access to critical section**
  - semaphore solution
  - lock solution
  
  ### Semaphore solution
  - **Semaphore solution uses integer variable (S) that is accessed through two standard atomic operations, the wait() and signal()**
  - The number of the integer variable (S) determines the number of processes that can have access to the critical section.

  **How semaphore solution works?**
  - Before a process can enter the critical section, it first checks the semaphore value
  - The process will be able to enter the critical section if the semaphore value is larger than 0
  - When the process enters the critical section, the semaphore value will decrease by 1 which prevents other process from entering the critical section.
  - When the process exits the critical section, the semaphore value will increase by 1, allowing a another process to access enter the critical section

  **The initial semaphore value is 1**
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/eb8fdd1d-2a63-4e60-9250-77951991b831)

  **Since the semaphore value is larger than 0, the process can enter the critical section. When the process enters the critical section, the semaphore value decreases by 1 which is now 0**
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/6d7cbe61-f562-4dd2-a3ff-f6b2542d79a1)

  **Since the semaphore value is now 0, the process B is not allowed to enter the shared critical section**
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/53a83ec1-76d8-4888-a7f6-09f908f8c2ac)

 ** When the process A finishes its execution, it will move through the exit section. The semaphore value is now increased by 1.**
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/5c9fa886-e3db-4c26-b17a-018840bbf786)

  **Since the semaphore value is 1, process B can now enter the critical section**
  ![image](https://github.com/Fong20/Learning-repository/assets/150316121/dcb3b5d9-2597-45de-b68a-dee58b5490a2)

  ### Lock solution
  - The lock solution is a simpler implementation to rectify race condition
  - It works by protecting the critical section with a lock. A process will need to first acquire the lock before it can access the critical section and only a lock can only be given to a process. If the lock is not available, the process will need to wait
  - The process will release the lock when it exits the critical section and the following next waiting process can enter the critical section.
## Process deadlock
Process deadlock occurs when one of the process is unable to obtain the resources to run
System table is used to record if the resource is free or which resource is allocated to a process

Process to request for resources
Request to Use to Release

If the process request for resources and the resources are not available, then the process will enter the waiting state.

Sometimes, a waiting process can never exit the waiting state as the resources it has requested are held by another waiting process. This will eventually cause deadlock

mutual exclusion = when two or more processes reuqest access ot a non sharable resource, only one process can have access to the resource

**Condiitons for deadlock to occur**
1. Mutual exclusion
2. hold and wait
3. no preemption
4. circular wait (confirm will cause deadlock)

### Resource allocation graph
- used to determine if deadlock will occur
- if the resource allocation graph has a cycle (circular wait), then the system is in deadlock state

![image](https://github.com/Fong20/Learning-repository/assets/150316121/d6ba0338-7601-4ac9-969e-daa21838c7f5)

Methods to handle deadlock
- deadlock prevention and avoidance
- deadlock detection and recovery (Leave it to the OS to handle it)
- ignore and restart

  ### Deadlock prevention
  - As stated above, deadlock occurs from the four conditons. The basic idea of deadlock prevention is to violate any one of the four conditions.
  - only condition 4 is possible whereas condition 1, 2 and 3 are not possible

  ### Deadlock avoidance
  Main purpose is to avoid unsafe allcation of resource

  ### Deadlock detection
  Deadlock detection is handled by the OS.

  claim-edge = future
