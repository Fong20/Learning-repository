# Artificial Neural Networks (ANN)
- Artificial neural networks (ANNs) are inspired from the human brain.
- Human brain consists of densely interconnected set of nerve cells called neurons (basic information-processing units).

dendrite to axon to synapse

- While responding to stimulations, neurons demonstrate long-term changes in the strength of their connections.
- New connections with other neurons maybe formed.
- This forms the basis for learning in the brain.
- Connections betweenneuronsleadingtothe‘rightanswer’arestrengthenedwhile those leadingtothe‘wronganswer’weaken.
- As a result, neural networks have the ability to learn through experience.

<img width="310" height="159" alt="image" src="https://github.com/user-attachments/assets/3f29311b-b256-492c-88d3-f90ca21dc1d7" />

## Training a perceptron
- In the process of learning, a set of data (i.e.input and output pairs) is used to train the weights of the connections.
- The weights are adjusted to reduce the difference between the actual and desired outputs of the perceptron.
- Both the single layer neural network and multilayer neural network.

<img width="845" height="383" alt="image" src="https://github.com/user-attachments/assets/eed382d0-5380-4ed7-87a1-b8fb5427b0e8" />

## Rosenblatt's Operation
The aim of the Rosenblatt’s perceptron is to classify the inputs, i.e.to group the inputs into two classes.

**Formula:**
- Input, x
- weights, w
- Linear combiner
- Hard limiter, θ. This is a constant value.
- Output, Y
  
<img width="593" height="183" alt="image" src="https://github.com/user-attachments/assets/3a482a1d-1087-471b-aca0-b354a0360139" />

- When the output is more than equal 0, it is positive.
- When the output is less than equal 0, it is negative.

## Hard Limiters

### AND Operation

### OR Operation

### XOR Operation
XOR Operation produces problems that are not linearly separable. It is not able to separate two classes

## Multilayer neural networks
A multilayer perceptron is a feedforward neural network with one or more hidden layers which is used to mitigate the issues found in XOR Operation.

The basic structure consists of

1. Input layer
2. Hidden layer
3. Output layer

<img width="395" height="189" alt="image" src="https://github.com/user-attachments/assets/94a5fc45-cacd-471c-8ef1-30290f152045" />

### Hidden Layer

### Back-propagation algorithm
One of the most poplular methods used in multilayer neural networks

similar tothatofaperceptron

1. traininginputispresentedtothenetworkinputlayer,
2. the networkpropagatestheinputfrominputlayertohiddenlayerandthentothe
output layerbeforegeneratingtheoutput
3. if thegeneratedoutputisdifferentfromthedesiredoutput,anerroriscalculated
and propagatedbackwardsfromtheoutputlayertotheinputlayer
4. the weightsareadjustedastheerrorispropagated

typically the sigmoid function is used as the activation function

<img width="474" height="328" alt="image" src="https://github.com/user-attachments/assets/1fbb6704-52f9-4000-87a3-0224d7a40850" />

<img width="847" height="402" alt="image" src="https://github.com/user-attachments/assets/b4aefd66-e889-4d40-9057-9bd50e246f02" />
