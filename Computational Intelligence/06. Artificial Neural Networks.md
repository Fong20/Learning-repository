# Artificial Neural Networks (ANN)
- Artificial neural networks (ANNs) are inspired from the human brain.
- Human brain consists of densely interconnected set of nerve cells called neurons (basic information-processing units).

dendrite to axon to synapse

While respondingtostimulations,neuronsdemonstratelong-termchangesinthe
strengthoftheirconnections.
▶ Newconnectionswithotherneuronsmaybeformed.
▶ This formsthebasisforlearninginthebrain.
▶ Connections betweenneuronsleadingtothe‘rightanswer’arestrengthenedwhile
those leadingtothe‘wronganswer’weaken.
▶ As aresult,neuralnetworkshavetheabilitytolearnthroughexperience.


<img width="310" height="159" alt="image" src="https://github.com/user-attachments/assets/3f29311b-b256-492c-88d3-f90ca21dc1d7" />

## Training a perceptron
- In the processoflearning,asetofdata(i.e.inputandoutputpairs)isusedtotrain the weightsoftheconnections.
- The weightsareadjustedtoreducethedifferencebetweentheactualanddesired
outputs oftheperceptron.

<img width="845" height="383" alt="image" src="https://github.com/user-attachments/assets/eed382d0-5380-4ed7-87a1-b8fb5427b0e8" />

## Rosenblatt's Operation
The aim of the Rosenblatt’s perceptron is to classify the inputs, i.e.to group the inputs into two classes.

Formula:

<img width="593" height="183" alt="image" src="https://github.com/user-attachments/assets/3a482a1d-1087-471b-aca0-b354a0360139" />

- Input, x
- weights, w
- Linear combiner
- Hard limiter, θ. This is a constant value.
- Output, Y

- When the output is more than equal 0, it is positive
- When the output is less than equal 0, it is negative

## Hard Limiters

### AND Operation

### OR Operation

### XOR Operation
XOR Operation produces problems that are not linearly separable. It is not able to separate two classes

## Multilayer neural networks
A multilayer perceptron is a feedforward neural network with one or more hidden layers which is used to mitigate the issues found in XOR Operation.

The basic structure consists of

1. Input layer
2. Hidden layer
3. Output layer

<img width="395" height="189" alt="image" src="https://github.com/user-attachments/assets/94a5fc45-cacd-471c-8ef1-30290f152045" />

### Hidden Layer

### Back-propagation algorithm
One of the most poplular methods used in multilayer neural networks

similar tothatofaperceptron

1. traininginputispresentedtothenetworkinputlayer,
2. the networkpropagatestheinputfrominputlayertohiddenlayerandthentothe
output layerbeforegeneratingtheoutput
3. if thegeneratedoutputisdifferentfromthedesiredoutput,anerroriscalculated
and propagatedbackwardsfromtheoutputlayertotheinputlayer
4. the weightsareadjustedastheerrorispropagated

typically the sigmoid function is used as the activation function

<img width="474" height="328" alt="image" src="https://github.com/user-attachments/assets/1fbb6704-52f9-4000-87a3-0224d7a40850" />

<img width="847" height="402" alt="image" src="https://github.com/user-attachments/assets/b4aefd66-e889-4d40-9057-9bd50e246f02" />
