# Artificial Neural Networks (ANN)
- Artificial neural networks (ANNs) are inspired from the human brain.
- Human brain consists of densely interconnected set of nerve cells called neurons (basic information-processing units).

dendrite to axon to synapse

- While responding to stimulations, neurons demonstrate long-term changes in the strength of their connections.
- New connections with other neurons maybe formed.
- This forms the basis for learning in the brain.
- Connections between neurons leading to the ‘right answer’ are strengthened while those leading to the ‘wronganswer’ weaken.
- As a result, neural networks have the ability to learn through experience.

<img width="310" height="159" alt="image" src="https://github.com/user-attachments/assets/3f29311b-b256-492c-88d3-f90ca21dc1d7" />

## Training a perceptron
- In the process of learning, a set of data (i.e.input and output pairs) is used to train the weights of the connections.
- The weights are adjusted to reduce the difference between the actual and desired outputs of the perceptron.
- Both the single layer neural network and multilayer neural network.

<img width="845" height="383" alt="image" src="https://github.com/user-attachments/assets/eed382d0-5380-4ed7-87a1-b8fb5427b0e8" />

## Rosenblatt's Operation
The aim of the Rosenblatt’s perceptron is to classify the inputs, i.e.to group the inputs into two classes.

**Formula:**
- Input, x
- weights, w
- Linear combiner
- Hard limiter, θ. This is a constant value.
- Output, Y
  
<img width="593" height="183" alt="image" src="https://github.com/user-attachments/assets/3a482a1d-1087-471b-aca0-b354a0360139" />

- When the output is more than equal 0, it is positive.
- When the output is less than equal 0, it is negative.

## Hard Limiters

### AND Operation

### OR Operation

### XOR Operation
XOR Operation produces problems that are not linearly separable. It is not able to separate two classes

## Multilayer neural networks
A multilayer perceptron is a feedforward neural network with one or more hidden layers which is used to mitigate the issues found in XOR Operation.

The basic structure consists of

1. Input layer
2. Hidden layer
3. Output layer

<img width="395" height="189" alt="image" src="https://github.com/user-attachments/assets/94a5fc45-cacd-471c-8ef1-30290f152045" />

### Hidden Layer

## Supervised ANN learning 
- The trainingdataincludestheinputdataandthedesiredoutputdata.
- The neuralnetworkistrainedusingthedifferencebetweenthecalculatedoutput and thedesiredoutputaserror.
- The aimoftheneuralnetworktrainingistominimisethediscrepancybetween the calculatedoutputandthedesiredoutput.

### Back-propagation algorithm
- One of the most poplular methods used in multilayer neural networks
- The algorithm works similar to that of training a perceptron.

**Process:**
1. training input is presented to the network input layer,
2. the network propagates the input from input layer to hidden layer and then to the output layer before generating the output
3. if the generated output is different from the desired output, an error is calculated and propagated backwards from the output layer to the input layer
4. the weights are adjusted as the error is propagated

typically the sigmoid function is used as the activation function

<img width="474" height="328" alt="image" src="https://github.com/user-attachments/assets/1fbb6704-52f9-4000-87a3-0224d7a40850" />

<img width="847" height="402" alt="image" src="https://github.com/user-attachments/assets/b4aefd66-e889-4d40-9057-9bd50e246f02" />

## Unsupervised ANN learning
- Also known as self-organised learning.
- Only input data is provided for the purpose of training. There is no desired output data provided.
- The neuralnetworkistrainedtodiscoversignificantfeaturesintheinput data/signals/patterns, andlearnhowtoclassifytheinputdataintodifferent categories.
- Self-organisingneuralnetworkslearnmuchfasterthanback-propagation
networks,thuscanbeusedinrealtime.
- Examples: Hebbian learning and competitive learning

### Hebian leaning
#### Hebb's Law
Hebb's law states that if neuron i is nearenoughto exciteneuron j and repeatedlyparticipatesinits
activation, the synaptic connection betweenthesetwoneuronsis strengthened and neuron j becomes moresensitivetostimulifromneuron i.

In ANN,Hebb’slawisrepresentedusingtworules:
1. If twoneuronsoneithersideofaconnectionareactivated synchronously, then the weightofthatconnectionis increased.
2. If twoneuronsoneithersideofaconnectionareactivated asynchronously, then the weightofthatconnectionis decreased.

<img width="741" height="322" alt="image" src="https://github.com/user-attachments/assets/3989f13a-85cf-4cd8-8c98-fd59ef685bad" />

### Competitive learning
In Hebbianlearning,severaloutputneuronscanbeactivatedsimultaneously.
▶ In competitivelearning,onlyasingleoutputneuronisactivatedatanytime.
▶ The outputneuronsthatwinsthe‘competition’iscalledthe winner-takes-all
neuron.
▶ Example: self-organisingfeaturemaps proposedbyKohonenin1989.

#### Kohonen network

<img width="741" height="286" alt="image" src="https://github.com/user-attachments/assets/092238df-940c-4268-b9a0-54270d97b5b6" />

## ANN Architectures 
1. Recurrent Neural Networks
2. Long Short Term Memory
3. Gated Recurrent Units

### Recurrent Neural Networks
- RecurrentNeuralNetworks(RNNs)areaclassofneuralnetworksdesignedfor sequential data.
- RNNs havefeedbackloopsthatallowinformationtopersist,makingthemcapableof rememberingpreviousinputsandusingthemtoinfluencefuturepredictions.
- BackpropagationThroughTime(BPTT)isusedtotrainRNNs. Similar aserror backpropagation,whereerrorsarepropagatedbackwards. The networkisunrolledacrosstimesteps,andthegradientsarecalculatedacrossthe sequence.

#### Strength
RNNs haveshort-termmemoryandareidealforhandlingsequenceswherethe relationshipbetweenthecurrentandpreviousstepsiscrucial.

### Limitations
- vanishing gradient
- exploding gradient problem = keeps on jumping within the graph without searching for the optimal solution
- size of the model, learning rate, and epoch causes vanishing/gradient problem

excessive weights will cause exploding gradient

### Long Short Term Memory (LSTM)
Long Short-TermMemory(LSTM)networksareaspecializedtypeofRNNdesigned to solvethevanishinggradientproblemandtocapturelong-termdependenciesmore effectively.

The coreofLSTMisitsmemorycell,whichcanretaininformationforlongperiods.It
also hasgatestocontroltheflowofinformation:
▶ ForgetGate
Decides whatinformationfromthecellstateshouldbediscarded.
▶ Input Gate
Decides whichinformationshouldbeupdatedinthecellstate.
▶ Output Gate
Decides whatinformationfromthecurrentcellshouldbepassedtothenexthiddenstate.

### Strength


### Gaterd Recurrent Units (GRU)
- Gated RecurrentUnits(GRU)areavariationofLSTMsthataimtosimplifythe model whilemaintainingsimilarperformance.
- GRUs combinetheforgetandinputgatesintoasinglegatecalledanupdategate, and theyeliminatetheoutputgate,simplifyingtheoverallarchitecture.

GRU has two main gates
▶ ResetGate rt
Determines howmuchoftheprevioushiddenstatetoforgetbeforecombiningitwiththenewinput.
▶ Update Gate zt
Decides howmuchoftheprevioushiddenstatetoretainandhowmuchofthenewinputto
incorporate.

#### Strength and Limitation
- GRUs aresimplerthanLSTMsbecausetheyhavefewergates.
- They tendtotrainfasterandmayworkaswellasLSTMsoncertaintasks.
- FewerparameterscomparedtoLSTM(sincefewergates),leadingtoasimpler model.
- Often fastertotrainduetofewercomputationspertimestep.
- ProvidessimilarperformancetoLSTMsformanytasks,especiallywith medium-length sequences.

## Designing a Neural Network
1. Data conditioning
2. Number of hidden layers
3. Number of neurons in hidden layers
4. Activation functions
5. Connections
6. Learning algorithm

### Data conditioning
Representationofinputdata(categorical,continuous,discrete).
▶ Data fortraining,testing,andvalidation.
the trainingdataisusedtotraintheneuralnetwork,
fromtimetotime,theperformanceoftheneuralnetworkisevaluatedusingthe
validationdata,
when theneuralnetworktrainingisterminated,theneuralnetworkistestedwiththe
testing data.
training,validation,andtestingdataareindependentofeachother.
the availabledataissplitintodifferentproportions.
trainingdatahasthelargestproportion;proportionsforvalidationandtestingdata
aresimilar.

### Number of hidden layers
Hidden layersareusefulinidentifyinghiddenfeaturesintheinputpatterns.
▶ Does itmeanthatweshoulduseasmanyhiddenneuronsaspossible?
▶ No. Overfittingmighthappen.

### Activation functions
As mentionedpreviously,therearedifferenttypeofactivationfunctions:linear,
sign, step,sigmoid.
▶ Generallyeachlayerwillhavethesametypeofactivationfunction.
▶ Activationfunctionsindifferentlayersarenotnecessarilythesametype.

### Connections
- Feedforward network
- Recurrent network

### Learning algorithms
- Supervised learning
- Unsupervised learning (Hebbian learning, Competitive learning)
