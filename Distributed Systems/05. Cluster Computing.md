# 05. Cluster Computing
- A group of interconnected computers (nodes) working together as a single powerful system, creating a unified computing resource far greater than the sum of its parts.
- By pooling resources, clusters enable high-performance computing capabilities that transform how we approach complex problems. The high performance, eg:Infiband allows the multiple computers to function as a single computer.
- This architecture provides the foundation for tackling intricate scientific challenges that single machines simply cannot handle efficiently or economically.

embarassingly parallel problem =  problem that is easily broken down into many independent tasks that can be processed simultaneously with little to no communication or dependency between them

## Components of Cluster Computing

### 1. Compute Nodes
- Individual computers equipped with CPUs/GPUs, memory, and local storage, forming the processing backbone

### 2. High-Speed Network
- InfiniBand or Ethernet interconnects enable rapid communication between nodes with minimal latency

### 3. Resource Manager / Cluster scheduler
- Orchestrates job distribution, monitors system health, and optimises resource allocation across the cluster
- Schedulers are the intelligent orchestrators of cluster computing, managing job queues, allocating resources, and strategically assigning tasks to nodes based on availability and priority.
- Their primary objectives include maximisingcluster utilisation, minimisinguser wait times, andensuring fair access across multiple research teams and projects.
- Modern schedulers elegantly handle diverse workloads—from quick interactive tasks and long-running batch jobs to always-on services—whilst adapting to changing demands in real time.
- eG: Slurm FairShare
  
### 4. Shared Storage
- Distributed file systems provide unified access to data, ensuring all nodes can retrieve and store information efficiently

## Uses of Cluster Computing

### 1. Climate Modelling
- Accelerates complex simulations predicting weather patterns and climate change scenarios decades into the future

### 2. Genomics Research
- Processes vast genetic datasets to unlock the secrets of disease, evolution, and personalisedmedicine

### 3. Astrophysics
- Analyses petabytes of cosmic data from observatories, revealing the universe's deepest mysteries

## Condor
HTCondor is a high-throughput computing (HTC) system and a batch-processing software suite used to manage and run large numbers of compute-intensive jobs. It works by collecting idle computers in a network and using their resources to run jobs submitted by users, maximizing hardware utilization. 

**High throughput computing, instead of high performance computing**
- less concerned about instantaneous computing power, but the amount of computing that can be harnessed over a month or a year

**all about match making**
- bring together the one who requests for computing power with those who offer them

### Important Condor concepts
1. Job
- the Condor representation of a piece of work
- Condor’s quanta of work
- Like a Unix process, and can be an element of a workflow

2. ClassAd
- representation / information of the resources

3. Machine or Resources
- computers that can do the processing

4. Pool
- A collection of computers used by Condor

5. Match Making
- Associating a job with a machine resource

6. Central Manager
- Central repository for the whole pool
- Does match making
- central manager must have negotiator and collector daemons, stard and schedd are optional

7. Submit Host
- The computer from which jobs are submitted to Condor

8. Execute Host
- The computer that runs a job
- The most powerful computing resources are always assigned to the execute host

### Condor daemons
1. condor_master:
- This program runs constantly and ensures that all other parts of Condor are running. If they hang or crash, it restarts them.

2. condor_collector:
- This program is part of the Condor central manager. It collects information about all computers in the pool as well as which users want to run jobs. It is what normally responds to the condor_status command.

3. condor_negotiator:
- This program is part of the Condor central manager. It decides what jobs should be run where.

4. condor_startd:
- If this program is running, it allows jobs to be started up on this computer--that is, your computer is an "execute machine". This advertises your computer to the central manager (more on that later, but in this case it's also your computer) so that it knows about this computer. It will start up the jobs that run.

5. condor_schedd
- If this program is running, it allows jobs to be submitted from this computer—that is, your computer is a "submit machine". This will advertise jobs to the central manager so that it knows about them. It will contact a condor_startd on other execute machines for each job that needs to be started.

6. condor_shadow (Not shown above)
- For each job that has been submitted from this computer, there is one condor_shadow running. It will watch over the job as it runs remotely. In some cases it will provide some assistance. You may or may not see any condor_shadow processes running, depending on what is happening on the computer when you try it out.
